{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from load_data import load_data\n",
    "\n",
    "# from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet model architecture initialisation\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, nbr_classes, Dropout):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.L1= nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11,stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.MaxPool2d(kernel_size=3, stride = 2)\n",
    "        )\n",
    "        self.L2= nn.Sequential(\n",
    "            nn.Conv2d(96,256,kernel_size= 5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(3,2)\n",
    "        )\n",
    "        self.L3= nn.Sequential(\n",
    "            nn.Conv2d(256,384,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(384)\n",
    "        )\n",
    "        self.L4= nn.Sequential(\n",
    "            nn.Conv2d(384,384,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(384)\n",
    "        )\n",
    "        self.L5= nn.Sequential(\n",
    "            nn.Conv2d(384, 256,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(3,2),\n",
    "        )\n",
    "        self.FC= nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(Dropout),\n",
    "            nn.Linear(256,4096),\n",
    "            nn.Dropout(Dropout),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.Dropout(Dropout),\n",
    "            nn.Linear(4096,nbr_classes),\n",
    "            nn.Softmax(nbr_classes)   # 15 persons for now just to test apres nzido     \n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        out = self.L1(data)\n",
    "        out = self.L2(out)\n",
    "        out = self.L3(out)\n",
    "        out = self.L4(out)\n",
    "        out = self.L5(out)\n",
    "        out = self.FC(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametres\n",
    "NC = 108\n",
    "epochs = 10\n",
    "batch_size = 30\n",
    "learning_rate = 0.01\n",
    "Dr = 0.2\n",
    "\n",
    "# Optimizer, loss\n",
    "model = AlexNet(nbr_classes= NC, Dropout= Dr).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 0.005, betas=[0.9,0.9])   # betas hna houwa momentum 9oli\n",
    "loss = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Load data\n",
    "trans = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "\n",
    "data = load_data(csv_file='casia.csv', transformer= trans)\n",
    "\n",
    "traind, testd = random_split(data, [700, 55])\n",
    "\n",
    "train_loader = DataLoader(traind, batch_size , shuffle= True)\n",
    "test_loader = DataLoader(testd, batch_size , shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1686, 0.1255, 0.1765,  ..., 0.1922, 0.2000, 0.2275],\n",
       "          [0.1412, 0.1608, 0.1882,  ..., 0.2275, 0.2235, 0.2157],\n",
       "          [0.1647, 0.1647, 0.1490,  ..., 0.2275, 0.2431, 0.2275],\n",
       "          ...,\n",
       "          [0.2941, 0.3333, 0.3451,  ..., 0.1490, 0.1686, 0.2157],\n",
       "          [0.2902, 0.3373, 0.3922,  ..., 0.1294, 0.1294, 0.1569],\n",
       "          [0.3333, 0.3490, 0.3686,  ..., 0.1451, 0.1373, 0.1529]]]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__getitem__(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Loss : 4.749927997589111\n",
      "Epoch [0/10], Loss : 222.2673797607422\n",
      "Epoch [0/10], Loss : 332.9625549316406\n",
      "Epoch [0/10], Loss : 971.3455200195312\n",
      "Epoch [0/10], Loss : 1665.962646484375\n",
      "Epoch [0/10], Loss : 563.7496948242188\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unsupported BMP pixel depth (264)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\Spé\\3eme_annee\\PFE\\PFE\\Code\\Final\\Classification\\AlexNet.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Sp%C3%A9/3eme_annee/PFE/PFE/Code/Final/Classification/AlexNet.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Sp%C3%A9/3eme_annee/PFE/PFE/Code/Final/Classification/AlexNet.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Sp%C3%A9/3eme_annee/PFE/PFE/Code/Final/Classification/AlexNet.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader): \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Sp%C3%A9/3eme_annee/PFE/PFE/Code/Final/Classification/AlexNet.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         images\u001b[39m=\u001b[39mimages\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Sp%C3%A9/3eme_annee/PFE/PFE/Code/Final/Classification/AlexNet.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         labels\u001b[39m=\u001b[39mlabels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32md:\\Spé\\3eme_annee\\PFE\\PFE\\Code\\Final\\Classification\\load_data.py:19\u001b[0m, in \u001b[0;36mload_data.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m (\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     18\u001b[0m     im_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsv\u001b[39m.\u001b[39miloc[index, \u001b[39m0\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(im_dir)\n\u001b[0;32m     20\u001b[0m     img\u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m, palette\u001b[39m=\u001b[39mImage\u001b[39m.\u001b[39mADAPTIVE)\n\u001b[0;32m     22\u001b[0m     label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsv\u001b[39m.\u001b[39miloc[index, \u001b[39m1\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\PIL\\Image.py:3133\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3130\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m   3131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 3133\u001b[0m im \u001b[39m=\u001b[39m _open_core(fp, filename, prefix, formats)\n\u001b[0;32m   3135\u001b[0m \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3136\u001b[0m     \u001b[39mif\u001b[39;00m init():\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\PIL\\Image.py:3119\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[1;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[0;32m   3117\u001b[0m \u001b[39melif\u001b[39;00m result:\n\u001b[0;32m   3118\u001b[0m     fp\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m-> 3119\u001b[0m     im \u001b[39m=\u001b[39m factory(fp, filename)\n\u001b[0;32m   3120\u001b[0m     _decompression_bomb_check(im\u001b[39m.\u001b[39msize)\n\u001b[0;32m   3121\u001b[0m     \u001b[39mreturn\u001b[39;00m im\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\PIL\\ImageFile.py:116\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[1;34m(self, fp, filename)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open()\n\u001b[0;32m    117\u001b[0m     \u001b[39mexcept\u001b[39;00m (\n\u001b[0;32m    118\u001b[0m         \u001b[39mIndexError\u001b[39;00m,  \u001b[39m# end of data\u001b[39;00m\n\u001b[0;32m    119\u001b[0m         \u001b[39mTypeError\u001b[39;00m,  \u001b[39m# end of data (ord)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m         struct\u001b[39m.\u001b[39merror,\n\u001b[0;32m    123\u001b[0m     ) \u001b[39mas\u001b[39;00m v:\n\u001b[0;32m    124\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mSyntaxError\u001b[39;00m(v) \u001b[39mfrom\u001b[39;00m \u001b[39mv\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\PIL\\BmpImagePlugin.py:274\u001b[0m, in \u001b[0;36mBmpImageFile._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    272\u001b[0m offset \u001b[39m=\u001b[39m i32(head_data, \u001b[39m10\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[39m# load bitmap information (offset=raster info)\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bitmap(offset\u001b[39m=\u001b[39;49moffset)\n",
      "File \u001b[1;32mc:\\Users\\21379\\anaconda3\\lib\\site-packages\\PIL\\BmpImagePlugin.py:167\u001b[0m, in \u001b[0;36mBmpImageFile._bitmap\u001b[1;34m(self, header, offset)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, raw_mode \u001b[39m=\u001b[39m BIT2MODE\u001b[39m.\u001b[39mget(file_info[\u001b[39m\"\u001b[39m\u001b[39mbits\u001b[39m\u001b[39m\"\u001b[39m], (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported BMP pixel depth (\u001b[39m\u001b[39m{\u001b[39;00mfile_info[\u001b[39m'\u001b[39m\u001b[39mbits\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[39m# ---------------- Process BMP with Bitfields compression (not palette)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m decoder_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mOSError\u001b[0m: Unsupported BMP pixel depth (264)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        \n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        #  img= img.unsqueeze(0)    # turns an n.d. tensor into an (n+1).d. one by adding an extra dimension of depth 1\n",
    "        \n",
    "        pred = model(images)\n",
    "        err= loss(pred, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch}/{epochs}], Loss : {err.item()}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 14 validation images: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy : {} %'.format(14, 100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model in pytorch\n",
    "model = models.alexnet()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
